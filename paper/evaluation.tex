\section{Evaluation}\label{sec:evaluation}

In this section, we experimentally analyze various real-time aspects
of the DeepPicar. This includes
(1) measurement based worst-case execution time (WCET) analysis of
deep learning inferencing,
(2) the effect of using multiple cores in accelerating inferencing,
(3) the effect of co-scheduling multiple deep neural network models,
and 
(4) the effect of co-scheduling memory bandwidth intensive co-runners,
and
(5) the effect of shared L2 cache partitioning and memory bandwidth
throttling for guaranteed real-time performance.

\subsection{Setup}
The Raspberry Pi 3 Model B platform used in DeepPicar equips a Broadcom
BCM2837 SoC, which has a quad-core ARM Cortex-A53 cluster,
running at up to 1.2GHz. Each core has private 32K I/D caches
and all cores share a 512KB L2 cache.
The chip also includes Broadcom's Videocore IV GPU, although we do
not use the GPU in our evaluation, due to the lack of sofware support
\footnote{TensorFlow currently only supports NVIDIA's GPUs.}.
For software, we use Ubuntu MATE 16.04 and TensorFlow 1.1.
We disable DVFS (dynamic voltage frequency scaling) and
configure the clock speed of each core statically at the maximum 1.2GHz.
We use the SCHED\_FIFO real-time scheduler to schedule the CNN control
task while using the CFS when executing memory intensive co-runners.

\subsection{Inference Timing for Real-Time Control}
For real-time control of a car (or any robot), the control loop
frequency must be sufficiently high so that the car can quickly
react to the changing environment and its internal states. In general,
control performance improves when the frequency is higher, though
computation time and the type of particular physical system are
factors in determining a proper control loop frequency. While a standard
control system may be comprised of multiple control loops with
differing control frequencies---e.g., an inner control loop for lower-level
PD control, an outer loop for motion planning, etc.---DeepPicar's
control loop is a single layer, as shown earlier in
Figure~\ref{fig:controlloop}, since a single deep neural network
replaces the traditional multi-layer control pipline. (Refer to
Figure~\ref{fig:end-to-end-control} on the differences between the
standard robotics control vs. end-to-end deep learning approach).
This means that the CNN inference operation must be completed
within the inner-most control loop update frequency.

To understand achievable control-loop update frequencies, we
experimentally measured the execution times of DeepPicar's CNN
inference operations.

% 50-200Hz for quadcopters:
% https://robotics.stackexchange.com/questions/231/what-frequency-does-my-quadcopter-output-sense-calculate-output-update-loop-need 
% https://quadmeup.com/pid-looptime-why-it-is-not-only-about-frequency/

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/Fig7_new}
  \caption{DeepPicar's control loop processing times over 1000 input image frames.}
  \label{fig:control-loop-timing}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular} {| c | r | r | r | r |}
    \hline
    \textbf{Operation} & \textbf{Mean} & \textbf{Max} &   \textbf{99pct.} & \textbf{Stdev.} \\ \hline
    Image capture        & 1.61  &  1.81 &  1.75  & 0.05 \\ \hline
    Image pre-processing & 2.77  &  2.90 &  2.87  & 0.04 \\ \hline
    CNN inferencing      & 18.49 & 19.30 & 18.99  & 0.20 \\ \hline
    Total Time           & 22.86 & 23.74 & 23.38  & 0.20 \\ \hline
  \end{tabular}
  \caption{Control loop timing breakdown.}
  \label{tbl:control-loop-breakdown}
\end{table}

Figure~\ref{fig:control-loop-timing} shows the measured control loop 
processing times of the DeepPicar over 1000 image frames (one per each
control loop). We omit the first frame's processing time for cache
warmup. Table~\ref{tbl:control-loop-breakdown} shows the time
breakdown of each control loop. Note that all four CPU cores of the
Raspberry Pi 3 were used by the TensorFlow library when performing the
CNN inference operations.

First, as expected, we find that the inference operation
dominates the control loop execution time, accounting for about 81\% of
the execution time.

Second, we find that the measured average
execution time of a single control loop is 22.86 ms, or 43.7 Hz and
the 99 percentile time is 23.38 ms.
This means that the DeepPicar can operate
at up to 40 Hz control frequency in real-time using only the on-board
Raspberry Pi 3 computing platform, as no remote computing resources 
were necessary. We consider these results surprising given the complexity
of the deep neural network, and the fact that the inference operation
performed by TensorFlow only utilizes the CPU cores of the Raspberry Pi 3.
In comparison, NVIDIA's DAVE-2 system, which has the exact same neural
network architecture, reportedly runs at 30 Hz~\cite{Bojarski2016}. 
Although we believe it was not
limited by their computing platform (we will experimentally compare
performance differences among multiple embedded computing platforms,
including NVIDIA's Jetson TX2, later in
Section~\ref{sec:comparison}), the fact that a low-cost
Raspberry Pi 3 can achieve comparable real-time control performance is
surprising.

Lastly, we find that the control loop execution timing is highly
predictable and shows very little variance over different input image
frames. This is because the amount of computation needed to perform
a CNN inferencing operation is fixed at the CNN architecture design
time and does not change at runtime over different inputs (i.e.,
different image frames). This predictable timing behavior is a highly
desirable property for real-time systems, making CNN inferencing an
attractive real-time workload.

\subsection{Effect of the Core Count to Inference Timing}

In this experiment, we investigate the scalability of performing
inference operations of DeepPicar's neural network with respect to the
number of cores. As noted earlier, the Raspberry Pi 3 platform has
four Cortex-A53 cores and TensorFlow 
provides a programmable mechanism to adjust how many cores are to be
used by the library. Leveraging this feature, we repeat the
experiment in the previous subsection but with using varying
numbers of CPU cores---from one to four.

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/perf_vs_corecnt}
  \caption{Average control loop execution time vs. \#of CPU
    cores.}
  \label{fig:perf-vs-corecnt}
\end{figure}

Figure~\ref{fig:perf-vs-corecnt} shows the average execution time of
the control loop as we vary the number of cores used by
TensorFlow. As expected, as we assign more cores, the average execution
time decreases---from 46.30 ms on a single core to 22.86 ms on four
cores (over a 100\% improvement). However, the improvement is far from an ideal
linear scaling. In particular, from 3 cores to 4 cores, the
improvement is a mere 2.80 ms, or 12\%. In short, we find that the
scalability of DeepPicar's deep neural network is not ideal.

As noted in~\cite{NVIDIA2015}, CNN inferencing is inherently more
difficult to parallelize than training because the easiest
parallelization option, batching (i.e., processing multiple images in
parallel), is not available or is limited. Specifically, in DeepPicar,
only one image frame, obtained from the camera, can be processed at a
time. Thus, more fine-grained algorithmic parallelization is needed to
improve inference performance~\cite{NVIDIA2015}, which generally does
not scale well. 

On the other hand, the limited scalability opens up the possibility of
consolidating multiple different tasks or different neural network
models rather than allocating all cores for a single neural network
model.
For example, it is conceivable to use four cameras and four different
neural network models, each of which is trained separately for a
different purpose and executed on a single dedicated core.
Assuming we use the same network
architecture for all models, then one might expect to achieve up to
20 Hz using one core (given that 1 core can deliver 46 ms average
execution time).
In the next experiment, we investigate the
feasibility of such a scenario.

\subsection{Effect of Co-scheduling Multiple CNN Models}

In this experiment, we launch multiple instances of DeepPicar's CNN
model at the same time and measure its impact on their inference
timings. In other words, we are interested in how shared resource
contention affects inference timing. For this, we create four different
neural network models, that have the same network architecture, and
run them simultaneously.
%% These instances are not identical
%% copies of the same model, but are instead copies of different models
%% (but of the same network architecture). 
%% This is done to ensure that no L2 cache memory is shared between the 
%% models as they are running.

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/perf_vs_modelcnt}
  \caption{Timing impact of co-scheduling multiple CNNs. 1Nx1C: one CNN
    model using one core; 4Nx1C: four CNN models each using one core;
    1Nx2C: one CNN model using two cores; 2Nx2C: two CNN models each
    using two cores.} 
  \label{fig:perf-vs-modelcnt}
\end{figure}

Figure~\ref{fig:perf-vs-modelcnt} shows the results. In the figure, the
X-axis shows the system configuration: \#of CNN models x \#of CPU
cores/CNN. For example, `4Nx1C' means running four CNN models each of
which is assigned to run on one CPU core, whereas `2Nx2C' means running
two CNN models, each of which is assigned to run on two CPU
cores. The Y-axis shows the average inference timing.
The two bars on the left show the impact of co-scheduling four CNN
models. Compared to executing a single CNN model on one CPU core
(1Nx1C), when four CNN models are co-scheduled (4Nx1C), each model
suffers an average inference time increase of approximately 11 ms,
or 24\%. On the other hand, when two CNN models, each using two CPU
cores, are co-scheduled (2Nx2C), the average inference timing is increased by
about 4 ms, or 13\%, compared to the baseline of running one model
using two CPU cores (1Nx2C). 

These increases in inference times in the co-scheduled scenarios are
expected because co-scheduled tasks on a
multicore platform interfere with each other due to contention in the
shared hardware resources, such as the shared
cache and DRAM~\cite{Gracioli2015,Yun2013}.

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.7\textwidth]{figs/l2missrate_vs_modelcnt}
%%   \caption{L2 cache miss rates of different neural network and core
%%     assignments. X-axis is the same as Figure~\ref{fig:perf-vs-modelcnt}.} 
%%   \label{fig:l2missrate-vs-modelcnt}
%% \end{figure}

%% To further analyze the source of contention, we use hardware
%% performance counters of the processor. Specifically, we measure L2
%% miss rates of the DNN models first in isolation and then after
%% co-scheduling other models. If the shared L2 cache is the primary
%% source of inteference, then the measured L2 miss rates will
%% increase. Figure~\ref{fig:l2missrate-vs-modelcnt} shows the results.
%% As can be see in the figure, L2 miss rates increase as more models are 
%% co-scheduled, but the increase is less than expected.
%% This suggests that the shared L2 cache is not the main bottleneck that 
%% causes execution time increases. In other words, DNN models don't appear 
%% to be significantly affected by the shared L2 cache space. 
%% % L2 partitioning is probably not going to be useful.
%% Instead, we hypothesize that it is likely caused by the memory
%% controller---the only other major shared hardware source---where
%% memory requests from different CPU cores contend, which would result
%% in increased memory access latency. While some Intel processors
%% provide incore hardware counters that can measure average memory
%% access latency~\cite{ye2016maracas}, we were not able to identify
%% whether such hardware counters exist in the BCM2837 processor of
%% Raspberry Pi 3 due to the lack of documentation. Instead, in the next
%% experiment, we use memory intensive synthetic benchmarks to test the
%% hypothesis.

\subsection{Effect of Co-scheduling Synthetic Memory Intensive
  Tasks}\label{sec:eval-memhog} 

In this experiment, we investigate the \emph{worst-case} impact of shared 
resource contention on DeepPicar's CNN inference timing using  
a synthetic memory benchmark. Specifically, we use the \emph{Bandwidth}
benchmark from the IsolBench suite~\cite{Valsan2016}, which
sequentially reads or writes a big array; we henceforth refer to BwRead 
as Bandwidth with read accesses and BwWrite as the one with write
accesses. The experiment setup is as follows: We run a single CNN
model on one core, and co-schedule an increasing number of the
Bandwidth benchmark instances on the other cores. We repeat the
experiement first with BwRead and next with BwWrite.

%% ~\footnote{\texttt{\$ bandwdith -a read  -m 16384 -t 1000}} 
%% ~\footnote{\texttt{\$ bandwdith -a write -m 16384 -t 1000}}.
  
\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/perf_vs_bandwidth}
  \caption{Average processing time vs. the number of memory
    intensive co-runners introduced.}
  \label{fig:perf_vs_bandwidth}
\end{figure}

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.45\textwidth]{figs/perf_vs_bandwidth_mem}
%%   \caption{Effect of memory performance hogs on the shared resources. 
%%     The DNN model uses Core 0 and memory-hog co-runners
%%     use the rest of the cores.}
%%   \label{fig:perf_vs_bandwidth_mem}
%% \end{figure}

Figure~\ref{fig:perf_vs_bandwidth} shows the results. Note
that BwWrite co-runners cause significantly higher execution time
increases---up to 11.6X---on CNN inferencing while BwRead co-runners
cause relatively much smaller time increases. While execution time
increases are expected, the degree to which that is seen in the worst-case 
is quite surprising---\emph{if the CNN controller was driving an actual 
car, it would result in a car crash!}

Given the importance of predictable timing for real-time control, such
as our CNN based control task, we wanted to: (1) understand the main
source of the timing and (2) evaluate existing isolation methods to
avoid this kind of timing interference. Specifically, shared cache
space and DRAM bandwidth are the two most well-known sources of contention
in multicore systems. Thus, in the following sections, we investigate whether
and to what extent they influence the observed timing interference and 
the effectiveness of existing mitigation methods.

%% To our surprise, we find that neither
%% the cache space nor DRAM bandwidth is the root cause of the
%% DeepPicar's observed worst-case execution time increase.

%% \fixme{story: is cache space contention the reason? traditional
%%   solution is cache partitioning. so, next we evaluate if cache
%%   partitioning can solve the problem. spolier alert: it does not.}

%% Figure~\ref{fig:perf_vs_bandwidth_mem} shows both the normalized
%% total memory bus accesses (first group) and L2 miss rates (second group).
%% In both figures, the DNN model is running on Core 0 as a function of the 
%% number of co-scheduled memory intensive synthetic benchmarks.
%% %% We also show 
%% %% the normalized total system bus access count (middle group), which is
%% %% representative of memory bandwidth usage of all the cores.
%% First, as we
%% increase the number of co-runners, the DNN model's execution times are
%% increased exponentially---by up to 11.6X---even though the DNN model
%% is running on a dedicated core (Core 0). On the other hand, the DNN
%% model's L2 cache-miss rates do not increase as much.
%% In other words, the DNN model's execution increase is not fully
%% explained by increased contention in the L2 cache space.
%% Instead, the exponential increase in the total number of bus accesses,
%% due to the memory-hog co-runners, appears to be more closely
%% correlated with the DNN model's execution time increase. This means
%% the high memory bandwidth pressure from the co-scheduled
%% memory intensive benchmarks increased the DNN model's
%% memory access latency.

%% Instead, as we hypothesized in the previous
%% experiment, the increased memory pressure from the co-scheduled memory
%% intensive benchmarks is likely the primary cause of the DNN model's execution
%% time increase. Therefore, we conclude that DeepPicar's DNN model is
%% more senstive to DRAM access latency than L2 cache space.

%% This observation suggests that shared cache partitioning
%% techniques~\cite{Gracioli2015,Kim2016} may not be effective isolation
%% solutions for DeepPicar's DNN processing workload, as cache
%% partitioning does not provide memory performance guarantees.

%% We do find the performance of the model in the presence of 
%% write co-runners surprising, as it has been previously shown in experiments
%% on another in-order ARM core, the Cortex A7, that write interference was not as 
%% significant\cite{Valsan2016}. We also tested the model, in the presence of write 
%% co-runners, on the A7 cores of the Odroid XU4 platform to see if  the same model 
%% behavior would also be seen there. The results can be seen in Figure ~\ref{fig:a53_vs_a7}. 
%% As expected, when the model is run on A7 cores, the intereference of the write
%% co-runners is not as impactful on model performance as it can still operate at 
%% under 10 Hz. 

%% Instead, memory controller focused isolation solutions,
%% either hardware or software-based ones (e.g.,~\cite{Guo2017,Yun2013}),
%% may be more important. Although our observation is made on a single
%% hardware platform running on a single DNN workload, we suspect that
%% many AI workloads may exhibit similar characteristics.

\input{evaluation-cachepart}
\input{evaluation-drampart}

%% \subsection{Summary of the Findings}
%% So far, we have evaluated DeepPicar's real-time
%% characteristics from the perspective of end-to-end deep learning based
%% real-time control, and made several observations.

%% First, we find that DeepPicar's computing platform, the Raspberry Pi
%% 3, offers sufficient computing capacity to perform deap-learning based
%% real-time control at 30 Hz frequency (or 33.3 ms per control
%% loop). Given the complexity of the DNN used, we were pleasantly
%% surprised by this finding. 
%% %% The time breakdown shows that the DNN inferencing operation, 
%% %% performed by the Tensorflow library, dominates the execution time, 
%% %% which is expected.

%% Second, we find that scalability of Tensorflow's DNN 
%% implementation is limited. We find that using all four cores can double 
%% performance of the CNN, but compared to using three cores, performance
%% improvement is relatively minimal.

%% Third, we find that consolidating multiple DNN models---on different CPU
%% cores---is feasible as we find: (1) DNN performance using a single
%% core is not much worse than using multiple cores; (2) multiple DNN
%% models running simultaneously cause moderate interference.

%% Fourth, we find that consolidating memory (DRAM) performance
%% intensive applications (especially write-heavy ones) can jeopadize
%% the DNN inferencing task's performance even when all tasks have
%% dedicated cores due to contention in shared hardware resources; we
%% observe up to 11.6X slowdown.

%% Lastly, we find that the DNN inferencing workload is not sensitive
%% to cache space and that cache partitioning is not effective in
%% providing timing isolation to our DNN workload. On the other hand, we
%% find that the DNN workload is sensitive to memory bandwidth and that
%% memory bandwidth throttling is effective in improving performance
%% isolation.

%% To the 
%% contrary, performance is constant regardless of the amount of cache 
%% space used. Furthermore, we find that performance remains consistent 
%% when memory intensive read co-runners are introduced, even though
%% the model has its own isolated cache space. As such, we 
%% conclude that the shared L2 cache is not essential to the DNN's real-time 
%% performance.

%% Lastly, we find that memory bandwidth throttling results in significant
%% changes to model performance. When less bandwidth is given to the model,
%% we find that it performs worse. When memory intensive co-runners are
%% present, we find that limiting the available bandwidth of the co-runners
%% results in significantly improved model execution times. As such, we conclude
%% that the shared DRAM memory is essential to the DNN's real-time 
%% performance.
