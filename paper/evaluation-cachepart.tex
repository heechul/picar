\subsection{Effect of Cache Partitioning}

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.7\textwidth]{figs/palloc_multicore_l2missrate}
%%   \caption{L2 miss rate impact of limiting the amount of L2 cache space
%%   available to the DNN.}
%%   \label{fig:palloc_multicore_l2missrate}
%% \end{figure}

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.7\textwidth]{figs/palloc_bandwidth_modelbus}
%%   \caption{Model bus access impact of co-scheduling memory intensive co-runners 
%% when cache partitioning is enabled.}
%%   \label{fig:palloc_bandwidth_modelbus}
%% \end{figure}

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.7\textwidth]{figs/palloc_bandwidth_bus}
%%   \caption{Total bus access impact of co-scheduling memory intensive co-runners 
%% when cache partitioning is enabled.}
%%   \label{fig:palloc_bandwidth_bus}
%% \end{figure}

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.7\textwidth]{figs/palloc_bandwidth_l2missrate}
%%   \caption{L2 miss rate impact of co-scheduling memory intensive co-
%% runners when cache partitioning is enabled.}
%%   \label{fig:palloc_bandwidth_l2missrate}
%% \end{figure}

Cache partitioning is a well-known technique to improve isolation in
real-time systems by giving a dedicated cache space to each individaul
task or core. In this experiment, we use PALLOC~\cite{yun2014rtas}, a
page-coloring based cache partitioning solution for Linux, to
investigate the effect of cache partitioning in protecting DeepPicar's
DNN based controller.

\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{figs/cache-mapping}
  \caption{Physical address mapping of L1/L2 caches of Broadcom
    BCM2837 processor in Raspberry Pi 3.}
  \label{fig:cache-mapping}
\end{figure}

Page coloring is a OS technique that controls the physical addresses
of memory pages. By allocating pages over non-overlapping sets of the
cache, the OS can effectively partition the cache.
Figure~\ref{fig:cache-mapping} shows the physical address
mapping of Raspberry Pi 3's BCM2837 processor, which has 32K private
L1 I\&D (4way) caches and a share 512KB L2 (16 way) cache. In order not
to partition the private L1 caches, we use the bit 13 and 14 for
coloring, which results in 4 usuable page colors.

%% determine if the DNN is mostly affected by the DRAM 
%% controller, we partition the L2 cache of the Pi3 and reperform the 
%% same experiments to see if there are any noticable changes. For the 
%% partition, we employ PALLOC~\cite{yun2014rtas}, a color-based page 
%% allocator that works on the kernel level.

%% For the bit mask, we select 
%% bits 12, 13 and 14 as they can be used to access the L2 cache,
%% as can be seen by Figure \ref{fig:cache-mapping}.
%% This results in 
%% 2\textsuperscript{3} = 8 colors, which we then assign to the Pi3's 
%% physical cores such that each core has two unique colors (colors 0 
%% and 1 are assigned to core 0, colors 2 and 3 are assigned to core 1, 
%% etc.). In the case of bit 12, since we use 2 colors for each partition,
%% the L1 Data cache is not partitioned in our experiments.

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/palloc_multicore}
  \caption{Cache space sensitivity of the DNN controller.}
  \label{fig:palloc_multicore}
\end{figure}

In the first experiment, we investigate the cache space sensitivity of
DeepPicar's DNN-based control loop. Using PALLOC, we create 4
different cgroups which are configured to use 4, 3,
2, and 1 colors (100\%, 75\%, 50\% and 25\% of the L2 cache
space, respectively). We then execute the DNN control loop (inference)
on each cache partition (cgroup) and measure the average processing
time.

Figure~\ref{fig:palloc_multicore} shows the result. As can be seen,
the DNN inference timing is hardly changed at all regardless of the
size of the allocated L2 cache space. In other words, we find that
the DNN workload is largely insensitive to L2 cache space.

The next experiment further validates this finding. In this
experiment, we repeat the experiment in
Section~\ref{sec:eval-memhog}---i.e., co-scheduling the DNN model and
three Bandwidth (BwRead or BwWrite) instances---but this time we
ensure that each task is given equal amounts of L2 cache space by
assigning one color to each task's cache partition.

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/palloc_bandwidth_exectime}
  \caption{Average processing time vs. the number of memory
memory intensive co-runners; Each core (task) is given a equal-sized
dedicated cache partition.}
  \label{fig:palloc_bandwidth_exectime}
\end{figure}

Figure \ref{fig:palloc_bandwidth_exectime} shows the
result. For BwRead, when cache partitioning is enabled, DNN
performance becomes slightly better, with a improvement of  
\textasciitilde6.5\%. For BwWrite, cache partitioning actually makes
the DNN performance slightly worse that that of w/o partitioning.

In summary, we find that the DNN inferencing workload is not sensitive
to cache space and that cache partitioning is not effective in
providing timing isolation to our DNN workload.

%% By partitioning the shared L2 cache, we find that no noticable 
%% improvements are gained and that performance remains consistent. 
%% In all experiements, the model shows no sensitivity to the shared L2 
%% cache. As a result, we conclude that cache partitioning is not an 
%% effective isolation mechanism, and that the performance of the shared 
%% DRAM controller is of greater importance to the real-time efficiacy of 
%% the DeepPicar.

%% If performance improves as a result of partitioning the shared cache 
%% then we know that the DNN, to some extent, relies on the shared cache 
%% in addition to the shared DRAM. On the other 
%% hand, if there is no improvement in performance, then it can be 
%% observed that the shared memory is more critical for the DNN.

%% We run the DNN model with different amounts of L2 cache being
%% available based on the number of colors being used. For example, if two 
%% colors are used, then 25\% of the L2 cache is available, whereas if all 
%% 8 colors are used, 100\% of the L2 cache is available. Particularly, we 
%% test the performance of the model when it utilizes a single core and
%% all four of the available cores. 

%% As expected, the amount of L2 cache 
%% available to the model does not result in any significant changes in 
%% performance. 
%% This can also be seen by the L2 miss rates of the model 
%% seen in \ref{fig:palloc_multicore_l2missrate}. As more cache space 
%% becomes avaiable, the percentage of L2 misses decreases, which doesn't 
%% affect the real-time performance of the DNN. As such, we find that a 
%% single model is not sensitive to the shared L2 cache.

%% \begin{figure}[h]
%%  \centering
%%   \includegraphics[width=.45\textwidth]{figs/palloc_multimodel}
%%   \caption{Timing impact of co-scheduling multiple DNNs when cache 
%% partitioning is enabled.}
%%   \label{fig:palloc_multimodel}
%% \end{figure}

%% We also co-schedule multiple models to see how cache partitioning affects
%% interference between them. Each model uses an equal number of colors, and
%% are thus given equal amounts of L2 cache space, and there is no overlap
%% in the colors used, so no L2 cache is shared between models. As can be 
%% seen in Figure \ref{fig:palloc_multimodel}, the performance of the models 
%% remains the same as when no cache partitioning was employed. Based of 
%% these results, we find that contention for the L2 cache is not the main
%% source of interference between multiple DNN models.

%% At the same time, though, the L2 miss rates of the model don't correlate 
%% to the timing increases seen, as is shown in 
%% \ref{fig:palloc_bandwidth_l2missrate}. Instead, we find that the number
%% of bus accesses increases when cache partitions are used, and they mirror
%% the timing increases of the model. This can be seen in Figure 
%% \ref{fig:palloc_bandwidth_bus}. These additional bus accesses are
%% most likely caused by the L2 prefetcher, as it can generate additional bus
%% accesses that aren't counted as L2 misses. As a result, we find that cache
%% partitioning is detrimental to the performance of the DNN so long as the 
%% 0L2 prefetcher is enabled (we have not yet found a way to disable it on
%% the Pi 3). Furthermore, no cache sensitivity was displayed by the model,
%% thus showing that the performance of the L2 cache is not vital for DNN 
%% performance when co-runners are introduced.

